{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入需要使用的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 使用GPU，如果有的话\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义一些使用到的超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "content='png/content.png'\n",
    "style='png/style.png'\n",
    "max_size=400\n",
    "total_step=2000\n",
    "log_step=10\n",
    "sample_step=500\n",
    "style_weight=100\n",
    "lr=0.003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建一个加载图片的helper函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path, transform=None, max_size=None, shape=None):\n",
    "    \"\"\"载入图片并转化为tensor\"\"\"\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    if max_size:\n",
    "        scale = max_size / max(image.size)\n",
    "        size = np.array(image.size) * scale\n",
    "        image = image.resize(size.astype(int), Image.ANTIALIAS)\n",
    "    \n",
    "    if shape:\n",
    "        image = image.resize(shape, Image.LANCZOS)\n",
    "    \n",
    "    if transform:\n",
    "        image = transform(image).unsqueeze(0)\n",
    "    \n",
    "    return image.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义一个图片预处理pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载风格图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = load_image(content, transform, max_size=max_size)\n",
    "style = load_image(style, transform, shape=[content.size(2), content.size(3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义VGG网络，这里使用`torchvision.models`进行创建，返回第0,5,10,19,28层作为特征。由于`pretrained=True`所以会下载weights文件，国内IP可能会出现如下错误：\n",
    "~~~bash\n",
    "Downloading: \"https://download.pytorch.......pth\" to C:\\Users\\......./.cache\\torch\\checkpoints\\resnet152-b121ed2d.pth\n",
    "\n",
    ".....\n",
    "\n",
    "urllib.error.URLError: <urlopen error [WinError 10054] 远程主机强迫关闭了一个现有的连接。>\n",
    "~~~\n",
    "\n",
    "这个是时候直接打开http链接下载到C:\\User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"Select conv1_1 ~ conv5_1 activation maps.\"\"\"\n",
    "        super(VGGNet, self).__init__()\n",
    "        self.select = ['0', '5', '10', '19', '28'] \n",
    "        self.vgg = models.vgg19(pretrained=True).features\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Extract multiple convolutional feature maps.\"\"\"\n",
    "        features = []\n",
    "        for name, layer in self.vgg._modules.items():\n",
    "            x = layer(x)\n",
    "            if name in self.select:\n",
    "                features.append(x)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义content为目标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = content.clone().requires_grad_(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "新建一个模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam([target], lr=lr, betas=[0.5, 0.999])\n",
    "vgg = VGGNet().to(device).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [10/2000], Content Loss: 7.4953, Style Loss: 23746.6035\n",
      "Step [20/2000], Content Loss: 15.1861, Style Loss: 21374.9062\n",
      "Step [30/2000], Content Loss: 22.3211, Style Loss: 18908.2129\n",
      "Step [40/2000], Content Loss: 28.8174, Style Loss: 16537.4043\n",
      "Step [50/2000], Content Loss: 34.8026, Style Loss: 14334.9492\n",
      "Step [60/2000], Content Loss: 40.2554, Style Loss: 12319.2197\n",
      "Step [70/2000], Content Loss: 45.2621, Style Loss: 10493.8389\n",
      "Step [80/2000], Content Loss: 49.8830, Style Loss: 8877.5225\n",
      "Step [90/2000], Content Loss: 54.0880, Style Loss: 7477.4932\n",
      "Step [100/2000], Content Loss: 57.8993, Style Loss: 6292.5620\n",
      "Step [110/2000], Content Loss: 61.3147, Style Loss: 5307.6030\n",
      "Step [120/2000], Content Loss: 64.3333, Style Loss: 4499.2031\n",
      "Step [130/2000], Content Loss: 66.9763, Style Loss: 3840.0518\n",
      "Step [140/2000], Content Loss: 69.2697, Style Loss: 3300.7930\n",
      "Step [150/2000], Content Loss: 71.2308, Style Loss: 2859.2915\n",
      "Step [160/2000], Content Loss: 72.9064, Style Loss: 2496.5149\n",
      "Step [170/2000], Content Loss: 74.3639, Style Loss: 2197.7297\n",
      "Step [180/2000], Content Loss: 75.6527, Style Loss: 1951.5302\n",
      "Step [190/2000], Content Loss: 76.8023, Style Loss: 1748.8800\n",
      "Step [200/2000], Content Loss: 77.8427, Style Loss: 1581.4004\n",
      "Step [210/2000], Content Loss: 78.7945, Style Loss: 1442.3505\n",
      "Step [220/2000], Content Loss: 79.6745, Style Loss: 1326.3467\n",
      "Step [230/2000], Content Loss: 80.4781, Style Loss: 1229.1094\n",
      "Step [240/2000], Content Loss: 81.2079, Style Loss: 1147.0564\n",
      "Step [250/2000], Content Loss: 81.8731, Style Loss: 1077.3595\n",
      "Step [260/2000], Content Loss: 82.4821, Style Loss: 1017.5443\n",
      "Step [270/2000], Content Loss: 83.0410, Style Loss: 965.8322\n",
      "Step [280/2000], Content Loss: 83.5633, Style Loss: 920.6299\n",
      "Step [290/2000], Content Loss: 84.0444, Style Loss: 880.5779\n",
      "Step [300/2000], Content Loss: 84.5035, Style Loss: 844.9026\n",
      "Step [310/2000], Content Loss: 84.9317, Style Loss: 812.7913\n",
      "Step [320/2000], Content Loss: 85.3248, Style Loss: 783.7379\n",
      "Step [330/2000], Content Loss: 85.6946, Style Loss: 757.2429\n",
      "Step [340/2000], Content Loss: 86.0455, Style Loss: 732.9131\n",
      "Step [350/2000], Content Loss: 86.3783, Style Loss: 710.4266\n",
      "Step [360/2000], Content Loss: 86.6936, Style Loss: 689.4959\n",
      "Step [370/2000], Content Loss: 86.9923, Style Loss: 669.9345\n",
      "Step [380/2000], Content Loss: 87.2763, Style Loss: 651.5983\n",
      "Step [390/2000], Content Loss: 87.5467, Style Loss: 634.3802\n",
      "Step [400/2000], Content Loss: 87.8023, Style Loss: 618.1508\n",
      "Step [410/2000], Content Loss: 88.0428, Style Loss: 602.8585\n",
      "Step [420/2000], Content Loss: 88.2764, Style Loss: 588.4150\n",
      "Step [430/2000], Content Loss: 88.5047, Style Loss: 574.6908\n",
      "Step [440/2000], Content Loss: 88.7209, Style Loss: 561.6218\n",
      "Step [450/2000], Content Loss: 88.9305, Style Loss: 549.1297\n",
      "Step [460/2000], Content Loss: 89.1301, Style Loss: 537.1974\n",
      "Step [470/2000], Content Loss: 89.3243, Style Loss: 525.7747\n",
      "Step [480/2000], Content Loss: 89.5075, Style Loss: 514.8075\n",
      "Step [490/2000], Content Loss: 89.6839, Style Loss: 504.2737\n",
      "Step [500/2000], Content Loss: 89.8566, Style Loss: 494.1531\n",
      "Step [510/2000], Content Loss: 90.0242, Style Loss: 484.4326\n",
      "Step [520/2000], Content Loss: 90.1876, Style Loss: 475.1038\n",
      "Step [530/2000], Content Loss: 90.3402, Style Loss: 466.1345\n",
      "Step [540/2000], Content Loss: 90.4886, Style Loss: 457.4929\n",
      "Step [550/2000], Content Loss: 90.6333, Style Loss: 449.1631\n",
      "Step [560/2000], Content Loss: 90.7728, Style Loss: 441.1292\n",
      "Step [570/2000], Content Loss: 90.9077, Style Loss: 433.3588\n",
      "Step [580/2000], Content Loss: 91.0350, Style Loss: 425.8608\n",
      "Step [590/2000], Content Loss: 91.1550, Style Loss: 418.6067\n",
      "Step [600/2000], Content Loss: 91.2714, Style Loss: 411.5746\n",
      "Step [610/2000], Content Loss: 91.3885, Style Loss: 404.7515\n",
      "Step [620/2000], Content Loss: 91.5033, Style Loss: 398.1198\n",
      "Step [630/2000], Content Loss: 91.6153, Style Loss: 391.6786\n",
      "Step [640/2000], Content Loss: 91.7261, Style Loss: 385.4249\n",
      "Step [650/2000], Content Loss: 91.8359, Style Loss: 379.3365\n",
      "Step [660/2000], Content Loss: 91.9437, Style Loss: 373.4294\n",
      "Step [670/2000], Content Loss: 92.0491, Style Loss: 367.6844\n",
      "Step [680/2000], Content Loss: 92.1533, Style Loss: 362.1193\n",
      "Step [690/2000], Content Loss: 92.2532, Style Loss: 356.7162\n",
      "Step [700/2000], Content Loss: 92.3509, Style Loss: 351.4631\n",
      "Step [710/2000], Content Loss: 92.4485, Style Loss: 346.3453\n",
      "Step [720/2000], Content Loss: 92.5454, Style Loss: 341.3650\n",
      "Step [730/2000], Content Loss: 92.6421, Style Loss: 336.5211\n",
      "Step [740/2000], Content Loss: 92.7339, Style Loss: 331.8096\n",
      "Step [750/2000], Content Loss: 92.8253, Style Loss: 327.2293\n",
      "Step [760/2000], Content Loss: 92.9171, Style Loss: 322.7668\n",
      "Step [770/2000], Content Loss: 93.0060, Style Loss: 318.4029\n",
      "Step [780/2000], Content Loss: 93.0952, Style Loss: 314.1368\n",
      "Step [790/2000], Content Loss: 93.1832, Style Loss: 309.9645\n",
      "Step [800/2000], Content Loss: 93.2685, Style Loss: 305.8857\n",
      "Step [810/2000], Content Loss: 93.3512, Style Loss: 301.8982\n",
      "Step [820/2000], Content Loss: 93.4358, Style Loss: 298.0059\n",
      "Step [830/2000], Content Loss: 93.5177, Style Loss: 294.2076\n",
      "Step [840/2000], Content Loss: 93.5985, Style Loss: 290.4972\n",
      "Step [850/2000], Content Loss: 93.6773, Style Loss: 286.8671\n",
      "Step [860/2000], Content Loss: 93.7539, Style Loss: 283.3155\n",
      "Step [870/2000], Content Loss: 93.8300, Style Loss: 279.8514\n",
      "Step [880/2000], Content Loss: 93.9032, Style Loss: 276.4688\n",
      "Step [890/2000], Content Loss: 93.9759, Style Loss: 273.1544\n",
      "Step [900/2000], Content Loss: 94.0506, Style Loss: 269.9031\n",
      "Step [910/2000], Content Loss: 94.1219, Style Loss: 266.7152\n",
      "Step [920/2000], Content Loss: 94.1907, Style Loss: 263.5869\n",
      "Step [930/2000], Content Loss: 94.2590, Style Loss: 260.5243\n",
      "Step [940/2000], Content Loss: 94.3305, Style Loss: 257.5288\n",
      "Step [950/2000], Content Loss: 94.4007, Style Loss: 254.5998\n",
      "Step [960/2000], Content Loss: 94.4689, Style Loss: 251.7312\n",
      "Step [970/2000], Content Loss: 94.5358, Style Loss: 248.9164\n",
      "Step [980/2000], Content Loss: 94.6017, Style Loss: 246.1619\n",
      "Step [990/2000], Content Loss: 94.6656, Style Loss: 243.4665\n",
      "Step [1000/2000], Content Loss: 94.7280, Style Loss: 240.8304\n",
      "Step [1010/2000], Content Loss: 94.7921, Style Loss: 238.2487\n",
      "Step [1020/2000], Content Loss: 94.8533, Style Loss: 235.7188\n",
      "Step [1030/2000], Content Loss: 94.9133, Style Loss: 233.2406\n",
      "Step [1040/2000], Content Loss: 94.9733, Style Loss: 230.8074\n",
      "Step [1050/2000], Content Loss: 95.0323, Style Loss: 228.4193\n",
      "Step [1060/2000], Content Loss: 95.0910, Style Loss: 226.0783\n",
      "Step [1070/2000], Content Loss: 95.1487, Style Loss: 223.7747\n",
      "Step [1080/2000], Content Loss: 95.2073, Style Loss: 221.5113\n",
      "Step [1090/2000], Content Loss: 95.2662, Style Loss: 219.2954\n",
      "Step [1100/2000], Content Loss: 95.3227, Style Loss: 217.1221\n",
      "Step [1110/2000], Content Loss: 95.3777, Style Loss: 214.9826\n",
      "Step [1120/2000], Content Loss: 95.4324, Style Loss: 212.8755\n",
      "Step [1130/2000], Content Loss: 95.4882, Style Loss: 210.8037\n",
      "Step [1140/2000], Content Loss: 95.5450, Style Loss: 208.7706\n",
      "Step [1150/2000], Content Loss: 95.6011, Style Loss: 206.7766\n",
      "Step [1160/2000], Content Loss: 95.6558, Style Loss: 204.8159\n",
      "Step [1170/2000], Content Loss: 95.7110, Style Loss: 202.8913\n",
      "Step [1180/2000], Content Loss: 95.7626, Style Loss: 201.0009\n",
      "Step [1190/2000], Content Loss: 95.8122, Style Loss: 199.1403\n",
      "Step [1200/2000], Content Loss: 95.8644, Style Loss: 197.3088\n",
      "Step [1210/2000], Content Loss: 95.9172, Style Loss: 195.5062\n",
      "Step [1220/2000], Content Loss: 95.9686, Style Loss: 193.7358\n",
      "Step [1230/2000], Content Loss: 96.0182, Style Loss: 191.9946\n",
      "Step [1240/2000], Content Loss: 96.0669, Style Loss: 190.2845\n",
      "Step [1250/2000], Content Loss: 96.1147, Style Loss: 188.6040\n",
      "Step [1260/2000], Content Loss: 96.1614, Style Loss: 186.9517\n",
      "Step [1270/2000], Content Loss: 96.2092, Style Loss: 185.3249\n",
      "Step [1280/2000], Content Loss: 96.2553, Style Loss: 183.7240\n",
      "Step [1290/2000], Content Loss: 96.3007, Style Loss: 182.1452\n",
      "Step [1300/2000], Content Loss: 96.3474, Style Loss: 180.5872\n",
      "Step [1310/2000], Content Loss: 96.3928, Style Loss: 179.0515\n",
      "Step [1320/2000], Content Loss: 96.4374, Style Loss: 177.5374\n",
      "Step [1330/2000], Content Loss: 96.4846, Style Loss: 176.0494\n",
      "Step [1340/2000], Content Loss: 96.5317, Style Loss: 174.5820\n",
      "Step [1350/2000], Content Loss: 96.5790, Style Loss: 173.1343\n",
      "Step [1360/2000], Content Loss: 96.6260, Style Loss: 171.7072\n",
      "Step [1370/2000], Content Loss: 96.6722, Style Loss: 170.3014\n",
      "Step [1380/2000], Content Loss: 96.7165, Style Loss: 168.9189\n",
      "Step [1390/2000], Content Loss: 96.7597, Style Loss: 167.5585\n",
      "Step [1400/2000], Content Loss: 96.8020, Style Loss: 166.2181\n",
      "Step [1410/2000], Content Loss: 96.8436, Style Loss: 164.8978\n",
      "Step [1420/2000], Content Loss: 96.8846, Style Loss: 163.5986\n",
      "Step [1430/2000], Content Loss: 96.9267, Style Loss: 162.3163\n",
      "Step [1440/2000], Content Loss: 96.9684, Style Loss: 161.0490\n",
      "Step [1450/2000], Content Loss: 97.0100, Style Loss: 159.7991\n",
      "Step [1460/2000], Content Loss: 97.0524, Style Loss: 158.5643\n",
      "Step [1470/2000], Content Loss: 97.0941, Style Loss: 157.3423\n",
      "Step [1480/2000], Content Loss: 97.1355, Style Loss: 156.1349\n",
      "Step [1490/2000], Content Loss: 97.1762, Style Loss: 154.9413\n",
      "Step [1500/2000], Content Loss: 97.2162, Style Loss: 153.7621\n",
      "Step [1510/2000], Content Loss: 97.2554, Style Loss: 152.5975\n",
      "Step [1520/2000], Content Loss: 97.2939, Style Loss: 151.4487\n",
      "Step [1530/2000], Content Loss: 97.3320, Style Loss: 150.3132\n",
      "Step [1540/2000], Content Loss: 97.3697, Style Loss: 149.1919\n",
      "Step [1550/2000], Content Loss: 97.4058, Style Loss: 148.0854\n",
      "Step [1560/2000], Content Loss: 97.4422, Style Loss: 146.9918\n",
      "Step [1570/2000], Content Loss: 97.4787, Style Loss: 145.9113\n",
      "Step [1580/2000], Content Loss: 97.5155, Style Loss: 144.8462\n",
      "Step [1590/2000], Content Loss: 97.5524, Style Loss: 143.7928\n",
      "Step [1600/2000], Content Loss: 97.5885, Style Loss: 142.7518\n",
      "Step [1610/2000], Content Loss: 97.6238, Style Loss: 141.7221\n",
      "Step [1620/2000], Content Loss: 97.6579, Style Loss: 140.7069\n",
      "Step [1630/2000], Content Loss: 97.6916, Style Loss: 139.7046\n",
      "Step [1640/2000], Content Loss: 97.7248, Style Loss: 138.7160\n",
      "Step [1650/2000], Content Loss: 97.7593, Style Loss: 137.7385\n",
      "Step [1660/2000], Content Loss: 97.7931, Style Loss: 136.7748\n",
      "Step [1670/2000], Content Loss: 97.8260, Style Loss: 135.8237\n",
      "Step [1680/2000], Content Loss: 97.8586, Style Loss: 134.8844\n",
      "Step [1690/2000], Content Loss: 97.8908, Style Loss: 133.9548\n",
      "Step [1700/2000], Content Loss: 97.9240, Style Loss: 133.0348\n",
      "Step [1710/2000], Content Loss: 97.9572, Style Loss: 132.1248\n",
      "Step [1720/2000], Content Loss: 97.9893, Style Loss: 131.2244\n",
      "Step [1730/2000], Content Loss: 98.0198, Style Loss: 130.3363\n",
      "Step [1740/2000], Content Loss: 98.0508, Style Loss: 129.4565\n",
      "Step [1750/2000], Content Loss: 98.0823, Style Loss: 128.5852\n",
      "Step [1760/2000], Content Loss: 98.1128, Style Loss: 127.7215\n",
      "Step [1770/2000], Content Loss: 98.1457, Style Loss: 126.8643\n",
      "Step [1780/2000], Content Loss: 98.1779, Style Loss: 126.0171\n",
      "Step [1790/2000], Content Loss: 98.2108, Style Loss: 125.1785\n",
      "Step [1800/2000], Content Loss: 98.2436, Style Loss: 124.3481\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-cbe4e514a88d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m# 提取特征，分别是0,5,10,19,28层的输出。\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtarget_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvgg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mcontent_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvgg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mstyle_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvgg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstyle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-85e0091d1e24>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvgg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                 \u001b[0mfeatures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 343\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[1;34m(self, input, weight)\u001b[0m\n\u001b[0;32m    338\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m    339\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[1;32m--> 340\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for step in range(total_step):\n",
    "\n",
    "    # 提取特征，分别是0,5,10,19,28层的输出。是5个元素的list\n",
    "    target_features = vgg(target)\n",
    "    content_features = vgg(content)\n",
    "    style_features = vgg(style)\n",
    "\n",
    "    style_loss = 0\n",
    "    content_loss = 0\n",
    "    # List中每个元素取出来计算\n",
    "    for f1, f2, f3 in zip(target_features, content_features, style_features):\n",
    "        # 计算目标图片与输入的loss\n",
    "        content_loss += torch.mean((f1 - f2)**2)\n",
    "\n",
    "        # 转化为卷积特征图\n",
    "        _, c, h, w = f1.size()\n",
    "        f1 = f1.view(c, h * w)\n",
    "        f3 = f3.view(c, h * w)\n",
    "\n",
    "        # 计算Gram矩阵\n",
    "        f1 = torch.mm(f1, f1.t())\n",
    "        f3 = torch.mm(f3, f3.t())\n",
    "\n",
    "        # 计算风格损失\n",
    "        style_loss += torch.mean((f1 - f3)**2) / (c * h * w) \n",
    "\n",
    "    # 损失加和并更新参数\n",
    "    loss = content_loss + style_weight * style_loss \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (step+1) % log_step == 0:\n",
    "        print ('Step [{}/{}], Content Loss: {:.4f}, Style Loss: {:.4f}' \n",
    "               .format(step+1, total_step, content_loss.item(), style_loss.item()))\n",
    "\n",
    "    if (step+1) % sample_step == 0:\n",
    "        # Save the generated image\n",
    "        denorm = transforms.Normalize((-2.12, -2.04, -1.80), (4.37, 4.46, 4.44))\n",
    "        img = target.clone().squeeze()\n",
    "        img = denorm(img).clamp_(0, 1)\n",
    "        torchvision.utils.save_image(img, 'output-{}.png'.format(step+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
